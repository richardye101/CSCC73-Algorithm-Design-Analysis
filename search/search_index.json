{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CSCC73","text":"<p>Notes from the CSCC73 course taken Winter 2023.</p> <p>Course Website: https://www.utsc.utoronto.ca/~bretscher/c73/index.html</p>"},{"location":"1-Greedy%20Algorithms/","title":"1 Greedy Algorithms","text":"<ul> <li>Djikstra's algorithm<ul> <li>Exchange argument proof<ul> <li>Directly to \\(v\\)</li> <li>Opt traverses more than a single edge to v</li> </ul> </li> </ul> </li> <li>Minimum lateness Problems<ul> <li>Optimal solution:<ul> <li>Proof idea: (Exchange argument)</li> </ul> </li> </ul> </li> <li>Data Compression</li> </ul>"},{"location":"1-Greedy%20Algorithms/#djikstras-algorithm","title":"Djikstra's algorithm","text":"<p>How to prove correctness?</p>"},{"location":"1-Greedy%20Algorithms/#exchange-argument-proof","title":"Exchange argument proof","text":"<p>Consider the edges of an optimal solution \\(O\\) ordered by how we added them in our greedy list of edges S</p> <p>\\(S = s_1, s_2, \\dots s_n\\)     Where for some \\(i\\), the first \\(i-1\\) items are the same as in \\(O\\)</p> <p>Let \\(opt = ((x,v), d_{opt}(v))\\) be the edge containing \\(v\\) with path distance \\(d_{opt}(v)\\) in \\(O\\)</p> <p>The optimal path may choose a different edge for the ith edge on the path to \\(v\\), or a path directly to \\(v\\).</p>"},{"location":"1-Greedy%20Algorithms/#directly-to-v","title":"Directly to \\(v\\)","text":"<ul> <li>The greedy algo will always choose the shortest path, but because it can't be shorter than the optimal solution (because its optimal) then they must be equal. This means we can do a swap and make the optimal solution more similar to the greedy one</li> </ul>"},{"location":"1-Greedy%20Algorithms/#opt-traverses-more-than-a-single-edge-to-v","title":"Opt traverses more than a single edge to v","text":"<ul> <li>Because the greedy algo will always choose the shortest path, then optimal cannot have more edges than the greedy algo</li> </ul> <p> This example is done where the greedy algo had a direct path to v from some ith state</p>"},{"location":"1-Greedy%20Algorithms/#minimum-lateness-problems","title":"Minimum lateness Problems","text":"<p>Need to schedule \\(n\\) jobs to minimize how late jobs finish overall in the schedule</p> <p>Ideas:</p> <ul> <li>Could do all the short jobs first?<ul> <li>Won't work because it ignores the deadlines for each job</li> </ul> </li> <li>Sort by increasing slack time (how much time there is between deadline and time required after start of schedule)</li> </ul>"},{"location":"1-Greedy%20Algorithms/#optimal-solution","title":"Optimal solution:","text":"<ul> <li>Will have no idle time (why have time in the schedule where nothing is being done?)</li> <li>No inversion (doing a task with a later deadline before a task with a earlier deadline)<ul> <li>If there is an inversion, there must be two consecutive jobs that are inverted.</li> </ul> </li> </ul>"},{"location":"1-Greedy%20Algorithms/#proof-idea-exchange-argument","title":"Proof idea: (Exchange argument)","text":"<ul> <li>Claim: All schedules with no inversion and no idle time have the same maximal lateness</li> <li>If there is an optimal soln with an inversion, you can swap the inversion and show the maximal lateness does not change. Any jobs before and after the two inverted items will not be affected as the time block will be the same.         - By flipping the inversion, we may have reduced the lateness of these two jobs, but the maximal lateness may have been caused by a different job, which means the maximal lateness doesn\u2019t change.</li> </ul>"},{"location":"1-Greedy%20Algorithms/#data-compression","title":"Data Compression","text":""},{"location":"2-Divide%20and%20Conquer/","title":"2 Divide and Conquer","text":"<ul> <li>Counting inversions<ul> <li>Definition</li> <li>How Divide and Conquer tackles inversions</li> <li>Proving time complexity of MergeSort<ul> <li>Proof</li> </ul> </li> </ul> </li> <li>Finding Closest Pairs<ul> <li>D&amp;C<ul> <li>Key to Divide and Conquer algorithm for Closest Pairs</li> </ul> </li> <li>Time Complexity</li> </ul> </li> <li>Matrix Multiplication<ul> <li>Strassens Algorithm<ul> <li>Complexity</li> <li>Q. How good does it get?</li> </ul> </li> </ul> </li> <li>Binary Multiplication</li> <li>Complexity when only splitting and multiplying three times</li> <li>Master theorem</li> <li> <p>In tutorial</p> </li> <li> <p>Split problem into smaller subproblems (usually equal size)</p> </li> <li>Solve each subproblem</li> <li>Combine solutions (usually in linear time)</li> <li>Often results in reduction of brute force, O(n2) algorithm to O(n log n) time</li> </ul>"},{"location":"2-Divide%20and%20Conquer/#counting-inversions","title":"Counting inversions","text":"<p>We use this method to measure how similar two lists are. </p>"},{"location":"2-Divide%20and%20Conquer/#definition","title":"Definition","text":"<p>List A: \\(a_1, a_2, \\dots, a_n\\) There is an inversion between item \\(i\\) and \\(j\\) if \\(i &lt; j\\) but \\(a_i &gt; a_j\\).</p> <p>There can be in total \\(O(n^2)\\) if we look at all nodes paired with all other nodes. By counting the inversions, we can obtain a measure of \"sortedness\" of a list.</p>"},{"location":"2-Divide%20and%20Conquer/#how-divide-and-conquer-tackles-inversions","title":"How Divide and Conquer tackles inversions","text":"<p>By dividing a list in half, we only need to compare inversions in each smaller list.   This algorithm will still take \\(O(n^2)\\)! To reduce it to \\(O(n log n)\\), we must sort the list after dividing in half.</p>"},{"location":"2-Divide%20and%20Conquer/#proving-time-complexity-of-mergesort","title":"Proving time complexity of MergeSort","text":"<p>Want to show that: \\(T(n) = 2(Tn/2) + O(n) + O(1) \\in O(n log n)\\)</p>"},{"location":"2-Divide%20and%20Conquer/#proof","title":"Proof","text":"\\[ \\begin{align*} T(n) &amp;= 2T(n/2) + cn \\quad \\text{Let } T(1) = a \\\\ T(n/2) &amp;= 2T(n/4) + c n/2 \\\\ T(n/4) &amp;= 2T(n/8) + c n/4 \\\\ \\vdots &amp; \\\\ T(n) &amp;= 2(2T(n/4) +c n/2) + cn\\\\ T(n) &amp;= 2(2(2T(n/8) + cn/4) + c n/2) + cn\\\\ &amp;= 2^3 T(n/2^3) + cn + cn + cn\\\\ &amp;=  2^3 T(n/2^3) + 3cn\\\\ T(n) &amp;= 2^i T(n/2^i) + icn\\\\ &amp; \\quad \\text{where eventually } n/2^i = 1 =&gt; i = log_2 n\\\\ &amp;= n a + c n log n \\end{align*} \\] <p>Or you could use the masters theorem, maybe it would be helpful to memorize this.</p>"},{"location":"2-Divide%20and%20Conquer/#finding-closest-pairs","title":"Finding Closest Pairs","text":"<p>For example, wanting to find the closest pair of points in an XY plane. </p> <p>The brute force solution is \\(O(n^2)\\) when comparing every single point with every other point.</p>"},{"location":"2-Divide%20and%20Conquer/#dc","title":"D&amp;C","text":"<ul> <li>Draw a line to separate the points, and compare closest points in each half.</li> <li>Combine by comparing every point from one side with the other</li> <li>Return the closest of the three sets of points</li> </ul>"},{"location":"2-Divide%20and%20Conquer/#key-to-divide-and-conquer-algorithm-for-closest-pairs","title":"Key to Divide and Conquer algorithm for Closest Pairs","text":"<p>You only need to check the 11 points below your starting point within the white lines.</p> <ul> <li>This means that the number of points youre checking is constant.</li> <li>There is no way you can have one point per box, because they must be \\(\\delta\\) apart and each box is size \\(\\delta/2\\) by \\(\\delta/2\\)</li> </ul> <p></p>"},{"location":"2-Divide%20and%20Conquer/#time-complexity","title":"Time Complexity","text":"<p> Same kind of proof as #Proving time complexity of MergeSort.</p>"},{"location":"2-Divide%20and%20Conquer/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Not really tested or need to know </p> <p>The brute force algorithm would take \\(O(n^3)\\)</p> <p>Trying to divide the matrices into n/2 blocks doesn't actually save any time.</p>"},{"location":"2-Divide%20and%20Conquer/#strassens-algorithm","title":"Strassens Algorithm","text":"<p>Reduce 8 multiplications to 7. Increase 4 additions to 18. </p>"},{"location":"2-Divide%20and%20Conquer/#complexity","title":"Complexity","text":"\\[ \\begin{align*} T(n) &amp;= 7T(n/2) + 18 O(n2) = 7(log n) + O(n2 log n)\\\\ &amp;\\implies 2^{2.81 log\\ n} + O(n^2 log n)\\\\ &amp;\\implies T(n) \\in O(n2.81) \\end{align*} \\]"},{"location":"2-Divide%20and%20Conquer/#q-how-good-does-it-get","title":"Q. How good does it get?","text":"<p>A. Best known algorithms:</p> <ul> <li>O(2.521813)(12/1979) </li> <li>O(2.521801)(01/1980) </li> <li>O(2.376)(1987)</li> </ul>"},{"location":"2-Divide%20and%20Conquer/#binary-multiplication","title":"Binary Multiplication","text":"<p>  Where the worst case complexity is \\(O(n^2)\\)</p>"},{"location":"2-Divide%20and%20Conquer/#complexity-when-only-splitting-and-multiplying-three-times","title":"Complexity when only splitting and multiplying three times","text":"<p> But replace the \\(n-1\\) in the geometric sum with \\(k\\), which is \\(log_2 (n-1)\\)</p> <p> </p>"},{"location":"2-Divide%20and%20Conquer/#master-theorem","title":"Master theorem","text":"<p>Useless if it's not in the correct format \\(aT(\\frac{n}{b}) + n^c\\)</p> <p></p> <p>Can get a bound on the floor/ceil, kinda complicated how its proved, don't need to derive ourselves by hand.  </p>"},{"location":"2-Divide%20and%20Conquer/#in-tutorial","title":"In tutorial","text":"<p>Attempt back substitution </p>"},{"location":"3-Order%20Statistics/","title":"3 Order Statistics","text":"<ul> <li>Refresher on Quicksort</li> <li>Quicksort For Order Statistics<ul> <li>How?</li> <li>Time complexity calculation</li> </ul> </li> <li>The Algorithm</li> </ul> <p>GIven an array \\(A\\) of \\(n\\) numbers, we want to return the \\(k^{th}\\) largest/smallest value (\\(k^{th}\\) order statistic).</p> <p>We don't want to sort, want better than \\(O(n log\\ n)\\) time, want \\(O(n)\\) time</p> <p>We could try Quicksort?</p>"},{"location":"3-Order%20Statistics/#refresher-on-quicksort","title":"Refresher on Quicksort","text":"<p>If its a sizeable array \\(A\\) of size \\(n\\), then we use a pivot to partition the array, and recursively split until you reach size 1</p>"},{"location":"3-Order%20Statistics/#quicksort-for-order-statistics","title":"Quicksort For Order Statistics","text":"<p>To use quicksort for this, we need to choose a good pivot. The pivot must:</p> <ul> <li>Reduce the input size significantly for the side we search in</li> <li>Want to pick the pivot efficiently</li> </ul>"},{"location":"3-Order%20Statistics/#how","title":"How?","text":"<ul> <li>Break \\(A\\) into constant size groups.<ul> <li>Sorting these constant size groups (lets say of size 5) is constant time, as you could try a variety of methods, that are constant time.</li> </ul> </li> <li>For each group find the median in constant time </li> <li>Recursively find the median of the medians<ul> <li></li> <li></li> <li>\\(3(\\frac{\\lfloor n/5 \\rfloor}{2}) = \\lfloor 3n / 10 \\rfloor\\) Which is the same for the number of elements larger than \\(X\\).<ul> <li>Where 3 is the number of values larger than the median in each group</li> <li>n/5 thats how many groups there are in total</li> <li>div by 2 because we're only looking at half the groups</li> </ul> </li> </ul> </li> <li>Until we have a pivot</li> </ul>"},{"location":"3-Order%20Statistics/#time-complexity-calculation","title":"Time complexity calculation","text":"<p>  Its only this bad as long as \\(n\\ge 50\\)</p>"},{"location":"3-Order%20Statistics/#the-algorithm","title":"The Algorithm","text":"<p>How can we prove the time complexity? We can't use the masters theory, but we can use induction.</p> <p>Let the \\(c\\) if \\(n &lt; 50\\) be a different constant \\(d\\) (her typo) </p>"},{"location":"4-Dynamic%20Programming/","title":"4 Dynamic Programming","text":"<p>Lets say we have another interval scheduling problem</p>"},{"location":"4-Dynamic%20Programming/#example","title":"Example","text":"<p>A sales person needs to schedule meetings with potential clients, and each meeting has a start and end time, and also a priority. In this case, its the profit for each meeting.</p> <p>We want to maximize profit in this scenario.</p> <p></p> <p>We can either include the last interval 8, or we schedule the last interval that is not 8 in the optimal solution.</p> <p></p>"},{"location":"4-Dynamic%20Programming/#potential-inefficient-algorithm","title":"Potential inefficient algorithm","text":"<p>This means we store the value of each prior call and see if we can improve it.</p> <p>Uses induction on the recurrence in the memoization to prove correctness</p>"},{"location":"4-Dynamic%20Programming/#how-to-know-to-use-dp","title":"How to know to use DP","text":"<p>Greedy: </p> <ul> <li>There is usually some sort of ordering, where the first choice (best choice) is enough to guarantee it is in the optimal solution</li> <li>Can proceed with the rest of the solution </li> </ul> <p>Divide and conquer:</p> <ul> <li>easier to do the problem when split into pieces</li> </ul> <p>Dynamic programming:</p> <ul> <li>Suppose you have all the optimal solution to subproblems, and you're asked to find the optimal solution that contains with the \\(n^{th}\\) value</li> <li>Kinda like the induction hypothesis, where you assume the \\(n-1\\) steps all work.</li> <li>Try greedy first, and if you can't solve it then you probably need to use dynamic programming</li> <li></li> </ul>"},{"location":"4-Dynamic%20Programming/#knapsack-problem-with-dp","title":"Knapsack problem with DP","text":"<p>We have items with value \\(v_{i}\\) and weight \\(w_{i}\\), and we can select items to put in the knapsack.</p> <p>Case 1: - Does not include the \\(i\\)th item - The optimal solution selects the best of \\(1,2, \\dots, i-1\\) items using weight limit \\(w\\) Case 2: - Include the \\(i\\)th item - new weight limit is \\(w-w_{i}\\) - Optimal solution selects best of \\(1,2, \\dots,(i-1)\\) using the new weight limit</p> <p> </p>"},{"location":"4-Dynamic%20Programming/#logic-behind-the-value-table","title":"Logic behind the value table","text":"<p>The way each value in the table is selected is by taking the max value of: - not including the new last node (look at the cell above) - including the new last node (look at the row above, cell of max weight = current weight - weight of last node)</p>"},{"location":"4-Dynamic%20Programming/#sequence-alignment","title":"Sequence alignment","text":""},{"location":"4-Dynamic%20Programming/#needleman-wunsch-algorithm","title":"Needleman-Wunsch Algorithm","text":""},{"location":"4-Dynamic%20Programming/#basic-process","title":"Basic process","text":"<p>For each pair of letters in each string, we either: - Match/mismatch (+2, -3) them to obtain a value of the left upper diagonal +2 or -3     - Ex. Take (row 2, col 3) = (C,T). They do not match, so we could simply take the upper left diag value of (A,C) - 3 = 0 - 3 = -3 - If they don't match, we can gap the first string (-2), meaning match the bottom string letter to a gap. This takes the value to the left and increments it.     - Ex. They do not match, so we can either gap the top letter T, resulting in the left value (C,C) - 2 = 2-2 = 0. - Gap the second string (-2), meaning match the top string letter to a gap. This takes the value above it and increments it.     - Ex. They do not match, so we can gap the bottom letter C, resulting in the top value (A,T) - 2 = -4 - 2 = -6. - \\(\\max\\{-3,0,-6\\}=0\\), so therefore (C,T) = 0.</p>"},{"location":"4-Dynamic%20Programming/#complexity","title":"Complexity","text":"<p>\\(O(mn)\\) time complexity \\(O(mn)\\) space complexity</p> <p>For english words/sentences, \\(m,n&lt;10\\)  But for computational biology, \\(m,n\\) could be 100,000, resulting in 10 billion operations! This needs to be improved.</p>"},{"location":"4-Dynamic%20Programming/#improving-the-sequence-alignment-problem","title":"Improving the sequence alignment problem","text":"<p>We can represent the table \\(M\\) as a directed acyclic graph \\(G_{M}\\). Each node in the graph is a subproblem.</p> <p>As we build the table \\(M\\) we can think of this as a directed acyclic graph \\(G_M\\). Each node represents a subproblem. For each \\(M[i, j]\\) we have a node \\((i, j)\\). Nodes \\(u_1, u_2, \\ldots, u_k \\rightarrow v\\) means subproblem \\(v\\) can only be solved once subproblems \\(u_1, u_2, \\ldots, u_k\\) have been solved. For the sequence alignment problem, this means that \\((i-1, j) \\rightarrow(i, j),(i, j-1) \\rightarrow(i, j)\\) and \\((i-1, j-1) \\rightarrow(i, j)\\). Can add the weight: - \\(\\delta\\) to \\((i-1, j) \\rightarrow(i, j)\\) and \\((i, j-1) \\rightarrow(i, j)\\)      - coming from previous mismatch/gap - \\(\\alpha\\) to \\((i-1, j-1) \\rightarrow(i, j)\\)     - coming from prev match</p> <p>What can we say about the optimal alignment and \\(G_{M}\\)?  A max(min) weight path from \\((0, 0)\\) to \\((m, n)\\) gives the optimal alignment.</p> <p></p> <p>We just need the columns \\(M[\\cdot, j]\\) and \\(M[\\cdot, j - 1]\\) to create an array \\(B\\) with two columns and \\(m\\) rows.</p> <p>Suppose we found the optimal value, but we don't know the path that we had to take to get there. We can use the following algorithm to obtain that path.</p> <p>We can use Divide and Conquer, by splitting the table in half</p> <p>First pass from (0,0) proof: For \\(i+j=0\\), we have \\(f(i, j)=f(0,0)=0=\\operatorname{Opt}(i, j)\\). For arbitrary \\(i, j\\), suppose that \\(f\\left(i^{\\prime}, j^{\\prime}\\right)=\\operatorname{Opt}\\left(i^{\\prime}, j^{\\prime}\\right)\\) for \\(i^{\\prime}+j^{\\prime}&lt;i+j\\). Consider the last edge on the longest path (highest weight) to \\((i, j)\\). By construction of the graph, this edge comes from either \\((i-1, j-1),(i-1, j)\\) or \\((i, j-1)\\). So $$ \\begin{aligned} f(i, j) &amp; =\\max \\left(\\alpha_{x_i y_i}+f(i-1, j-1), \\delta+f(i-1, j), \\delta+f(i, j-1)\\right) \\ \\text { by I.H } &amp; =\\max \\left(\\alpha_{x_i y_i}+\\operatorname{Opt}(i-1, j-1), \\delta+\\operatorname{Opt}(i-1, j), \\delta+\\operatorname{Opt}(i, j-1)\\right) \\ &amp; =\\operatorname{Opt}(i, j) \\end{aligned} $$</p> <p>Then we want to walk through from the bottom (m,n) </p> <p></p>"},{"location":"4-Dynamic%20Programming/#complexity_1","title":"Complexity","text":"<p>So the path (longest path just means path with highest weight) will just have a complexity of: - \\(O(mn)\\) time: computing \\(g(\\cdot, j)\\)  - \\(O(m + n)\\) space.</p>"},{"location":"4-Dynamic%20Programming/#hirschbergs-algorithm","title":"Hirschberg\u2019s Algorithm","text":"<p>Divide: Find the index \\(q\\) that maximizes \\(f(q, n/2) + g(q, n/2)\\) using our Space Efficient Algorithm and save \\((i, j) = (q, n/2)\\) as part of our solution.  Align \\(x_{q}\\) with \\(y_{n/2}\\) . </p> <p>Conquer: Recursively compute optimal alignment on each piece.</p> <p></p> <p></p>"},{"location":"4-Dynamic%20Programming/#space-complexity","title":"Space complexity","text":""},{"location":"4-Dynamic%20Programming/#time-complexity","title":"Time complexity","text":"<p>  ^ Read over the proof a bit more, key condition is that \\(k\\geq \\frac{c}{2}\\)</p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/","title":"5 DP Bellman Ford Algorithm","text":"<p>An algorithm that works on graphs with negative weights. </p> <p>From Djikstra's, we could possibly add a constant to all edges to make them all positive. However, if the shortest path (by weight) uses many edges, then the weight would have been increased for the entire path. </p> <p>Negative weight graphs with cycles could cause a cycle and endlessly loop to decrease weight.</p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/#how-it-works","title":"How it works","text":"<p>Build paths backward from a given node \\(T\\). We can start by finding the shortest (least weight) path with one edge to \\(T\\). Then we can repeat with two edges to \\(T\\). When we repeat with three edges, if we find a better path then we update the node to \\(T\\) with that path.</p> <p>This algorithm may run for \\(n-1\\) iterations, OR until two iterations where the shortest path does not decrease (need to be more clear).</p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/#recursive-function","title":"Recursive Function","text":"\\[ OPT(i,v) = \\text{length of shortest } v\\to t \\text{ path } P \\text{ using at most } i \\text{ edges.} \\] <p>Cases: - The shortest \\(v-t\\) path uses at most \\(i-1\\) edges. - The shortest path uses \\(OPT(i-1,u) + w(v,u)\\) for some \\(u\\in N_{out}(v)\\)      - \\(min_{u\\in N_{out}(v)}(OPT(i-1,u) + w(v,u))\\)     - Where \\(N_{out}(v)\\) is the out neighbourhood of \\(v\\)</p> <p></p> <p>This function will end up being \\(O(n^{3})\\) time because for every node, it loops \\(1\\to n\\) edge lengths and for each edge length it will look at every edge connected to that node which in worst case is \\(n\\).</p> <p>It will be \\(O(n^{2})\\) space as each iteration of edge length, from \\(1\\to (n-1)\\), each \\(n\\) nodes  have some shortest path value.</p> <p></p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/#improving-spacetime-complexity","title":"Improving Space/Time Complexity","text":"<p>Improving Space Complexity We don't need to store every single column to make this work, just the \\(i-1\\) iteration.</p> <p>This means we only need to store: - the adjacency list of each node (in order to determine the in and out neighbours for each node) which is \\(m\\) edges - They current column of the table, \\(M[v]\\), and the successor \\(s[v]\\) (the \\(N_{out}\\) of \\(v\\)?) which in total is \\(O(n)\\)</p> <p>Therefore, space complexity is reduced to \\(O(m+n)\\)</p> <p>Improving Time Complexity We don't need to check \\(O(n)\\) nodes for every entry \\(M(i,v)\\), just the nodes which are in the out neighbourhood of \\(v\\) (\\(N_{out}[v]\\))</p> <p></p> <p>We will only compare up to \\(O(m)\\) edges, resulting in \\(O(nm)\\) time complexity.</p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/#improvement-push-values-ahead","title":"Improvement: Push values ahead","text":"<ul> <li>Notice that at each step of the algorithm we perform a fetch action to retrieve \\(OPT(i \u2212 1, v)\\) or \\(OPT(i \u2212 1, u)\\). </li> <li>If we can instead push these values ahead each time, we can skip the nodes whose values won\u2019t change. </li> <li>Initially when \\(i = 0\\) only \\(t\\) has a finite value - it is only reachable from itself. This means all paths using 1 edge must be of the form \\((u, t)\\).</li> <li>So the iteration when \\(i = 0\\), need only check those vertices \\(u \\in V\\) such that \\(t \\in N_{out}(u)\\), i.e., (u, t).</li> </ul> <p>Basically, if we know a node \\(u\\) takes less than \\(i\\) edges to get to the goal node \\(t\\), then we can just push it's weight to the next column for \\(i+1\\) edges. In a subsequent iteration, if we find that we can reach \\(t\\) taking a path from \\(u\\to v\\to t\\), then we flag it as something we updated and have to check in a future iteration, and update it's optimal weight.</p> <p></p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/#negative-cycles","title":"Negative Cycles","text":"<p>Since Bellman Ford works for graphs with negative edge weights, what if there exists some negative cycle?</p> <p>Reasonably, that implies we could have a path longer than the number of nodes \\(n\\) which would then be deemed the shortest/least weight path from \\(s\\to t\\).  $$ \\implies OPT(i,v) \\text{ decreases as } i \\geq n  $$</p> <p>All we need to do is check that \\(OPT(n,v) = OPT(n-1,v)\\), where \\(n\\) is the number of nodes in the graph.</p> <p>Logically, this just means that if the shortest/least weight path from \\(s\\) to \\(t\\) contains more than \\(n-1\\) edges, the weight will not decrease. If it does, then we have found a negative weight cycle somewhere.</p> <p>Claim: There is no negative cycle on a path to \\(t \\iff\\) \\(OPT(n,v) = OPV(n-1,v)\\) for all nodes \\(v\\)</p>"},{"location":"5-DP-Bellman-Ford%20Algorithm/#proof-that-the-bellman-ford-has-no-cycles","title":"Proof that the Bellman Ford has no cycles","text":"<p>Need to prove the iff</p> <p> The version two of the proof for the forward direction </p> <p>Reverse/Backward Direction of the IFF </p>"},{"location":"6-DP-Floyd-Warshall/","title":"6 DP Floyd Warshall","text":"<p>Bellman ford finds the shortest path from all nodes to some goal node \\(t\\) with negative edges. It takes \\(O(n^{3})\\) time in the worst case. </p> <p>Idk why this was mentioned but: Dealing with sinks: A sink is a node  \\(t\\) where all nodes are directed to it and nothing flows out of \\(t\\)</p>"},{"location":"6-DP-Floyd-Warshall/#why-floyd-warshall","title":"Why Floyd-Warshall","text":"<p>This algorithm floyd-warshall is a way to find all-pair shortest paths, where all-pair shortest paths finds the shortest paths between each pair of node. It is still \\(O(n^{3})\\) time in worst case though. You wouldn't wanna do this until you wanted access to all the pairs.</p> <p>To do this with bellman ford, we would have to compute the algorithm for each vertex/node \\(n\\) resulting in a time complexity of \\(O(n^{4})\\) in the worst case.</p>"},{"location":"6-DP-Floyd-Warshall/#in-bellman-ford","title":"In Bellman Ford:","text":"<ul> <li>Suppose a shortest path from \\(v_{1}\\to v_{5}\\) is \\(v_{1},v_{8},v_{2},v_{3},v_{5}\\), then the shortest path from \\(v_{1}\\to v_{2}\\) and \\(v_{2}\\to v_{5}\\) are already known, because by definition bellman ford will have already taken the shortest path through theses nodes.</li> </ul>"},{"location":"6-DP-Floyd-Warshall/#method","title":"Method","text":"<ol> <li>Order the vertices \\(v_{1}, v_{2}, \\dots, v_{n}\\) and consider them in order.</li> <li>For each pair \\((v_{i},v_{j})\\), the initial shortest path is the weight of the edge between them, if it exists. Otherwise we set it to \\(\\infty\\). (Basically just the adjacency matrix)</li> <li>On the first iteration \\(i=1\\), we consider including vertex \\(v_{1}\\) on the shortest path.<ol> <li>Is there an edge \\((v_{i},v_{1})\\) and a \\((v_{1}, v_{j})\\) edge such that their sum is less than the weight between \\((v_{i}, v_{j})\\)?</li> </ol> </li> <li>On the second iteration, we look if there are any shorter paths containing \\(v_{1}\\) and \\(v_{2}\\).</li> </ol> <p>Basically, </p> <p>Perhaps at each iteration, when we look at each pair \\(i,j\\), we also allow the algorithm to look at the \\(1,\\dots ,k\\)  adjacency lists</p> <p>We can define the recurrence relation as: </p>"},{"location":"6-DP-Floyd-Warshall/#example","title":"Example","text":"<p>For detecting cycles: If the weight from a given node to itself decreases, then we know we have found a negative cycle. AKA Just check the diagonal of the table</p>"},{"location":"6-DP-Floyd-Warshall/#pseudocode","title":"Pseudocode","text":""},{"location":"6-DP-Floyd-Warshall/#proving-correctness","title":"Proving correctness","text":""},{"location":"7-Optimal-BST/","title":"7 Optimal BST","text":"<p>Something or other idk man</p>"},{"location":"8-MaxFlow-MinCut-Ford_Fulkerson/","title":"8 MaxFlow MinCut Ford Fulkerson","text":"<p>Network flow is represented by a directed graph, where there is a single source and a single sink. There are many edges which represent how much flow can go through that edge.</p> <p></p> <p>Essentially, we want to calculate the maximum flow (units) we can send from the source to the sink through the weighted edges.</p> <p>Capacity limit: Let \\(e\\in E\\) represent an edge, and the flow \\(f(e)\\) over edge \\(e\\) is bounded by \\(0\\leq f(e)\\leq c(e)\\) where \\(c(e)\\) is the capacity of the edge.</p> <p>Conservation of flow: For each vertex \\(v\\in V-{s,t}\\), we must have: $$ \\sum_{e \\text{ into } v}f(e) = \\sum_{e \\text{ out from }v}f(e) $$</p> <p>The value of flow \\(v(f) = \\sum_{e \\text{ out of }s}f(e)\\) represents the total flow coming out of \\(s\\) (the source). This is what we want to maximize. To do this we can solve an equivalent problem of determining which edges, if cut from the graph, disconnect s and t completely and have the lowest weight/capacity. This the min-cut algorithm which can be used to determine the max-flow.</p> <p>To do this, we introduce the concept of a residual graph. It represents how much capacity each edge has left after each run of the following algorithm. At the beginning, it is simply a copy of the original graph.</p> <p>For the edge which has been maxed out in each iteration, we introduce a new reverse edge of the original capacity representing how much flow we can push back across that edge.</p> <p>The algorithm is as follows: - Initialize a flow value variable \\(v\\) which will represent the flow coming out of \\(s\\). - Take a graph and run a DFS algorithm for paths from \\(s\\to t\\), and store the path - Find the minimum weight edge on the path (the bottleneck).  - Remove that weight from all edges on this path in the residual graph.     - For the bottleneck edge, reduce to 0 and      - Introduce a backward edge of the same weight. - Increment \\(v\\) by the bottleneck weight - Repeat algorithm, which will find another, different path. - End algorithm when there is no path from \\(s\\to t\\)</p> <p>At the end, we will have a residual graph where the set of vertices connected to \\(s\\) will represent the min-cut A, and we can obtain the flow over any edge in the original graph by subtracting the capacity by the residual capacities (ignoring backward edges).</p> <p></p> <p>In the end we should have a cut where capacity of the cut is exactly equal to the flow value out of \\(s\\).</p>"},{"location":"9-Midterm-Notes/","title":"9 Midterm Notes","text":"<p>It was kinda hard for me. </p> <p>It asked about the hoffman encoding, which I forgot for the most part, except the fact you need to split into two groups of the variables to encode. It asked for two algorithms that can detect negative weight cycles, which I answered Bellman-Ford and Floyd-Warshall. It's to be seen if i'm correct. It asked for a greedy proof, which I think I got; it was for minimizing the total average cost, where the avg cost is computed at each i as i increases to n. The next question asked for an algorithm that can determine the number of intersections of lines in a box. I answered some type of mergesort, although I didn't have time to code it out. Got stuck for a while :. It proceeded to ask for a DP solution the maximum subarray problem, which I think i got although I didnt clarify it properly, the recurrence cases depended on value i didn't explicitly calculate anywhere. Don't think that one went too well. Last one (i think) asked for a DP solution to a crazy graph, where you need to find the independence set of nodes with the maximum value. Barely answered the first question proving a greedy algorithm is not optimal. Anyway, I can cr/ncr so nbd.</p>"}]}